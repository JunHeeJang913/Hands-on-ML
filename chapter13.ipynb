{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(5, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(7, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2) # Items : [0,2,4,6,8,10,12]....\n",
    "dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...\n",
    "dataset = dataset.filter(lambda x: x < 10) # Items : 0, 2, 4, 6, 8, 0, 2, ...\n",
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>MedianHouseValue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.5214</td>\n      <td>15.0</td>\n      <td>3.049945</td>\n      <td>1.106548</td>\n      <td>1447.0</td>\n      <td>1.605993</td>\n      <td>37.63</td>\n      <td>-122.43</td>\n      <td>1.442</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.3275</td>\n      <td>5.0</td>\n      <td>6.490060</td>\n      <td>0.991054</td>\n      <td>3464.0</td>\n      <td>3.443340</td>\n      <td>33.69</td>\n      <td>-117.39</td>\n      <td>1.687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.1000</td>\n      <td>29.0</td>\n      <td>7.542373</td>\n      <td>1.591525</td>\n      <td>1328.0</td>\n      <td>2.250847</td>\n      <td>38.44</td>\n      <td>-122.98</td>\n      <td>1.621</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.1736</td>\n      <td>12.0</td>\n      <td>6.289003</td>\n      <td>0.997442</td>\n      <td>1054.0</td>\n      <td>2.695652</td>\n      <td>33.55</td>\n      <td>-117.70</td>\n      <td>2.621</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0549</td>\n      <td>13.0</td>\n      <td>5.312457</td>\n      <td>1.085092</td>\n      <td>3297.0</td>\n      <td>2.244384</td>\n      <td>33.93</td>\n      <td>-116.93</td>\n      <td>0.956</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "#skip --> 헤더행 건너뛰기\n",
    "#n_reader : 5개의 파일 에서 각 행을 섞는다.\n",
    "#interleave() 병렬 사용안함.  각 파일에서 그저 한 줄씩 읽을 뿐, 그러나 병렬적으로 읽고싶다면, \n",
    "# num_parallel_calls 인자를 원하는 쓰레드 수로 설정하면 된다.\n",
    "# tf.data.experimental.AUTOTUNE을 사용하면 텐서플로우가 \n",
    "# 알아서 동적으로 가용한 CPU에서 알맞은 쓰레드의 수를 구해준다.\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418'\nb'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0'\nb'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\nb'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\nb'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n이미 훈련셋의 각 특성에 대해 평균과 표준편차를 구했다고 가정.\\npreprocessing() 함수는 한 줄에 대해 파싱을 시작한다. decode_csv(파싱할라인, 각 열에대한 기본값을 갖고있는 배열)함수\\ndecode_csv함수는 열 당 하나의 스칼라 텐서를 반환한다. 그러나 1차원 텐서 배열이 필요하기 때문에, tf.stack()함수를 호출하였다.\\nreturn문에서는 스케일링 하였다.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    defs = [0.]*n_inputs + [tf.constant([], dtype= tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x-X_mean) / X_std, y\n",
    "\n",
    "\"\"\"\n",
    "이미 훈련셋의 각 특성에 대해 평균과 표준편차를 구했다고 가정.\n",
    "preprocessing() 함수는 한 줄에 대해 파싱을 시작한다. decode_csv(파싱할라인, 각 열에대한 기본값을 갖고있는 배열)함수\n",
    "decode_csv함수는 열 당 하나의 스칼라 텐서를 반환한다. 그러나 1차원 텐서 배열이 필요하기 때문에, tf.stack()함수를 호출하였다.\n",
    "return문에서는 스케일링 하였다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_reader=5, \n",
    "                        n_read_thread = None, shuffle_buffer_size = 10000, \n",
    "                        n_parse_thread = 5, batch_size = 32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath : tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_thread)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_thread)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(b'this is the first record', shape=(), dtype=string)\ntf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"this is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "\n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type = \"GZIp\")\n",
    "\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"this is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "\n",
    "filepaths = [\"my_compressed.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  }
 ]
}